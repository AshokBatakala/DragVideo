{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG DATA DIR \n",
    "\n",
    "DIR_NAME = \"EXP_test\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing - dlib:\n",
    "\n",
    "#pti :\n",
    "\n",
    "PTI_DIR = \"/home/bean/DragVideo/PTI\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.chdir(PTI_DIR)\n",
    "sys.path.append(PTI_DIR)\n",
    "\n",
    "from utils.align_data import pre_process_images\n",
    "from utils.align_data import paths_config\n",
    "\n",
    "path_raw_img = \"/home/bean/DragVideo/Data_store/data/raw_images\" # Path to raw images\n",
    "paths_config.input_data_path = '/home/bean/DragVideo/Data_store/data/processed_images_sg3' # save path for pre-processed images\n",
    "\n",
    "# preprocessing\n",
    "pre_process_images(path_raw_img, IMAGE_SIZE=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/ipykernel_launcher.py', '--ip=127.0.0.1', '--stdin=9008', '--control=9006', '--hb=9005', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"efbb559f-5634-4d2b-a88b-40e2fae93df5\"', '--shell=9007', '--transport=\"tcp\"', '--iopub=9009', '--f=/home/bean/.local/share/jupyter/runtime/kernel-v2-311u85kkGMUtssd.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/bean/DragVideo/PTI/pti_preprocess.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'pti_preprocess.py', '/home/bean/DragVideo/Data_store/data/raw_images', '/home/bean/DragVideo/Data_store/data/processed_images_sg3'], returncode=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "os.chdir(\"/home/bean/DragVideo/PTI\")\n",
    "\n",
    "import pti_preprocess\n",
    "\n",
    "path_raw_img = \"/home/bean/DragVideo/Data_store/data/raw_images\" # Path to raw images\n",
    "input_data_path = '/home/bean/DragVideo/Data_store/data/{DIR_NAME}/processed_images_sg3' # save path for pre-processed images\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(input_data_path):\n",
    "\tos.makedirs(input_data_path)\n",
    "\n",
    "# define paths here\n",
    "# give paths through cmd args\n",
    "subprocess.run(['python','pti_preprocess.py',path_raw_img, input_data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "current_directory:  /home/bean/DragVideo/PTI\n",
      "current_directory:  /home/bean/DragVideo/PTI\n",
      "torch.Size([3, 256, 256]) 3 256 256\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 107, in forward\n      _48 = torch.__not__(torch.__contains__(_47, \"conv1\"))\n      if _48:\n        x5 = (_22).forward(x4, )\n              ~~~~~~~~~~~~ <--- HERE\n      else:\n        x5 = x4\n  File \"code/__torch__.py\", line 447, in forward\n    _192 = torch.to(self.bias, ops.prim.dtype(x), False, False, None)\n    _193 = self.padding\n    x49 = torch.conv2d(x, _191, _192, [1, 1], [_193, _193], [1, 1], 1)\n          ~~~~~~~~~~~~ <--- HERE\n    x50 = __torch__.torch.nn.functional.relu(x49, False, )\n    return x50\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 103, in forward\n                    ys.append(x.to(torch.float32))\n                if name not in ['pool5', 'fc1', 'fc2', 'fc3', 'softmax']:\n                    x = layer(x)\n                        ~~~~~ <--- HERE\n            for idx, (y, w) in enumerate(zip(ys, [self.lpips0, self.lpips1, self.lpips2, self.lpips3, self.lpips4])):\n                y = y / (torch.norm(y, dim=1, keepdim=True) + 1e-10)\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 18, in forward\n    def forward(self, x):\n        x = torch.nn.functional.conv2d(x, self.weight.to(x.dtype), self.bias.to(x.dtype), padding=self.padding)\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        x = torch.nn.functional.relu(x)\n        return x\nRuntimeError: GET was unable to find an engine to execute this computation\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import os\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# os.chdir(\"/home/bean/DragVideo/PTI\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrun\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m run\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/DragVideo/PTI/run.py:116\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(encoder_path, stylegan_path, save_tuned_SG_path, save_latents_path, use_multi_id_training, verbose)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msave_latents_path:\u001b[39m\u001b[39m'\u001b[39m,paths_config\u001b[39m.\u001b[39membedding_base_dir)\n\u001b[1;32m    115\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun_pti\u001b[39;00m \u001b[39mimport\u001b[39;00m run_PTI\n\u001b[0;32m--> 116\u001b[0m model_id \u001b[39m=\u001b[39m run_PTI(use_wandb\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, use_multi_id_training\u001b[39m=\u001b[39;49muse_multi_id_training)\n\u001b[1;32m    118\u001b[0m generator_type \u001b[39m=\u001b[39m paths_config\u001b[39m.\u001b[39mmulti_id_model_type \u001b[39mif\u001b[39;00m use_multi_id_training \u001b[39melse\u001b[39;00m image_name\n\u001b[1;32m    119\u001b[0m old_G, new_G \u001b[39m=\u001b[39m load_generators(model_id, generator_type)\n",
      "File \u001b[0;32m~/DragVideo/PTI/scripts/run_pti.py:42\u001b[0m, in \u001b[0;36mrun_PTI\u001b[0;34m(run_name, use_wandb, use_multi_id_training)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     coach \u001b[39m=\u001b[39m SingleIDCoach(dataloader, use_wandb)\n\u001b[0;32m---> 42\u001b[0m coach\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m global_config\u001b[39m.\u001b[39mrun_name\n",
      "File \u001b[0;32m~/DragVideo/PTI/training/coaches/multi_id_coach.py:39\u001b[0m, in \u001b[0;36mMultiIDCoach.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m     embedding_dir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mw_path_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mpaths_config\u001b[39m.\u001b[39mpti_results_keyword\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mimage_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     37\u001b[0m os\u001b[39m.\u001b[39mmakedirs(embedding_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 39\u001b[0m w_pivot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_inversion(w_path_dir, image_name, image)\n\u001b[1;32m     40\u001b[0m w_pivots\u001b[39m.\u001b[39mappend(w_pivot)\n\u001b[1;32m     41\u001b[0m images\u001b[39m.\u001b[39mappend((image_name, image))\n",
      "File \u001b[0;32m~/DragVideo/PTI/training/coaches/base_coach.py:66\u001b[0m, in \u001b[0;36mBaseCoach.get_inversion\u001b[0;34m(self, w_path_dir, image_name, image)\u001b[0m\n\u001b[1;32m     63\u001b[0m     w_pivot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_inversions(w_path_dir, image_name)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m hyperparameters\u001b[39m.\u001b[39muse_last_w_pivots \u001b[39mor\u001b[39;00m w_pivot \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     w_pivot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalc_inversions(image, image_name)\n\u001b[1;32m     67\u001b[0m     torch\u001b[39m.\u001b[39msave(w_pivot, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00membedding_dir\u001b[39m}\u001b[39;00m\u001b[39m/0.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m w_pivot \u001b[39m=\u001b[39m w_pivot\u001b[39m.\u001b[39mto(global_config\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/DragVideo/PTI/training/coaches/base_coach.py:93\u001b[0m, in \u001b[0;36mBaseCoach.calc_inversions\u001b[0;34m(self, image, image_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     id_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze((image\u001b[39m.\u001b[39mto(global_config\u001b[39m.\u001b[39mdevice) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m---> 93\u001b[0m     w \u001b[39m=\u001b[39m w_projector\u001b[39m.\u001b[39;49mproject(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mG, id_image, device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(global_config\u001b[39m.\u001b[39;49mdevice), w_avg_samples\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m,\n\u001b[1;32m     94\u001b[0m                             num_steps\u001b[39m=\u001b[39;49mhyperparameters\u001b[39m.\u001b[39;49mfirst_inv_steps, w_name\u001b[39m=\u001b[39;49mimage_name,\n\u001b[1;32m     95\u001b[0m                             use_wandb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_wandb)\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m w\n",
      "File \u001b[0;32m~/DragVideo/PTI/training/projectors/w_projector.py:73\u001b[0m, in \u001b[0;36mproject\u001b[0;34m(G, target, num_steps, w_avg_samples, initial_learning_rate, initial_noise_factor, lr_rampdown_length, lr_rampup_length, noise_ramp_length, regularize_noise_weight, verbose, device, use_wandb, initial_w, image_log_step, w_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m target_images\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m256\u001b[39m:\n\u001b[1;32m     72\u001b[0m     target_images \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(target_images, size\u001b[39m=\u001b[39m(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marea\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m target_features \u001b[39m=\u001b[39m vgg16(target_images, resize_images\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_lpips\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     75\u001b[0m w_opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(start_w, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m     76\u001b[0m                      requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m     77\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([w_opt] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(noise_bufs\u001b[39m.\u001b[39mvalues()), betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m),\n\u001b[1;32m     78\u001b[0m                              lr\u001b[39m=\u001b[39mhyperparameters\u001b[39m.\u001b[39mfirst_inv_lr)\n",
      "File \u001b[0;32m~/.conda/envs/stylegan3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 107, in forward\n      _48 = torch.__not__(torch.__contains__(_47, \"conv1\"))\n      if _48:\n        x5 = (_22).forward(x4, )\n              ~~~~~~~~~~~~ <--- HERE\n      else:\n        x5 = x4\n  File \"code/__torch__.py\", line 447, in forward\n    _192 = torch.to(self.bias, ops.prim.dtype(x), False, False, None)\n    _193 = self.padding\n    x49 = torch.conv2d(x, _191, _192, [1, 1], [_193, _193], [1, 1], 1)\n          ~~~~~~~~~~~~ <--- HERE\n    x50 = __torch__.torch.nn.functional.relu(x49, False, )\n    return x50\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 103, in forward\n                    ys.append(x.to(torch.float32))\n                if name not in ['pool5', 'fc1', 'fc2', 'fc3', 'softmax']:\n                    x = layer(x)\n                        ~~~~~ <--- HERE\n            for idx, (y, w) in enumerate(zip(ys, [self.lpips0, self.lpips1, self.lpips2, self.lpips3, self.lpips4])):\n                y = y / (torch.norm(y, dim=1, keepdim=True) + 1e-10)\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 18, in forward\n    def forward(self, x):\n        x = torch.nn.functional.conv2d(x, self.weight.to(x.dtype), self.bias.to(x.dtype), padding=self.padding)\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        x = torch.nn.functional.relu(x)\n        return x\nRuntimeError: GET was unable to find an engine to execute this computation\n"
     ]
    }
   ],
   "source": [
    "# # import os\n",
    "# # os.chdir(\"/home/bean/DragVideo/PTI\")\n",
    "# import run\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/bean/DragVideo/PTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_path: /home/bean/DragVideo/Data_store/model_weights/restyle_e4e_ffhq.pt\n",
      "stylegan_path: /home/bean/DragVideo/Data_store/model_weights/sg3_3rdtime_weights/sg3-r-ffhq-1024_module.pkl\n",
      "save_tuned_SG_path: /home/bean/DragVideo/Data_store/data/PTI_results/checkpoints\n",
      "save_latents_path: /home/bean/DragVideo/Data_store/data/PTI_results/embeddings\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "current_directory:  /home/bean/DragVideo/PTI\n",
      "current_directory:  /home/bean/DragVideo/PTI\n",
      "torch.Size([3, 1024, 1024]) 3 1024 1024\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() profile_node %106 : int = prim::profile_ivalue(%104)\n",
      " does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "100%|██████████| 500/500 [03:32<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting large updated pickle based off new generator and ffhq.pkl\n",
      "--------------------------------\n",
      "model .pkl file name: TQVCVHVTJBUI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/bean/DragVideo/PTI\")\n",
    "\n",
    "from run import run\n",
    "# sys.path.append('./')\n",
    "!export PYTHONPATH=$PYTHONPATH:$PWD\n",
    "#sys.path.append('')\n",
    "# encoder path\n",
    "#e4e_path ='/home/bean/DragVideo/Data_store/model_weights/restyle_e4e_ffhq.pt'\n",
    "e4e_path ='/home/bean/DragVideo/Data_store/model_weights/restyle_e4e_ffhq.pt'\n",
    "\n",
    "#stylegan checkpoints\n",
    "stylegan2_ada_ffhq_path = '/home/bean/DragVideo/Data_store/model_weights/sg3_3rdtime_weights/sg3-r-ffhq-1024_module.pkl'\n",
    "\n",
    "#paths for saving chkpts and latents\n",
    "SG_checkpoints_dir = '/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints' # tuned_stylegan_weights\n",
    "embedding_base_dir = '/home/bean/DragVideo/Data_store/data/PTI_results/embeddings' # latents\n",
    "\n",
    "run(e4e_path, \n",
    "      stylegan2_ada_ffhq_path,\n",
    "      SG_checkpoints_dir,\n",
    "      embedding_base_dir, verbose=True) # arg: return : true -> latent,tuned_sg\n",
    "\n",
    "      # SG(latent) --> image\n",
    "      # draggan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bean/.conda/envs/stylegan3/bin:/home/bean/.vscode-server/bin/704ed70d4fd1c6bd6342c436f1ede30d1cff4710/bin/remote-cli:/home/bean/.conda/envs/stylegan3/bin:/opt/conda/condabin:/opt/conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['PATH'])\n",
    "#\"/home/bean/.conda/envs/stylegan3/bin:/home/bean/.vscode-server/bin/704ed70d4fd1c6bd6342c436f1ede30d1cff4710/bin/remote-cli:/home/bean/.conda/envs/stylegan3/bin:/opt/conda/condabin:/opt/conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n",
    "#/home/bean/.conda/envs/stylegan3/bin:/home/bean/.vscode-server/bin/704ed70d4fd1c6bd6342c436f1ede30d1cff4710/bin/remote-cli:/home/bean/.conda/envs/stylegan3/bin:/opt/conda/condabin:/opt/conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark_path:  /home/bean/DragVideo/Data_store/data/PTI_results/landmarks/000.pkl\n"
     ]
    }
   ],
   "source": [
    "#get landmarks\n",
    "os.chdir('/home/bean/DragVideo/facial-landmarks-recognition') # give path of facial landmarks folder\n",
    "\n",
    "from main import landmarks, dict_landmarks,show_landmarks,get_landmarks_dir\n",
    "\n",
    "base_dir = \"/home/bean/DragVideo/\"\n",
    "landmarks_dir = base_dir+\"Data_store/data/PTI_results/landmarks\" # here it stores landmarks .pkls\n",
    "processed_images_dir = base_dir + \"Data_store/data/processed_images_sg3\"\n",
    "\n",
    "#load an image from processed_images\n",
    "image_names= os.listdir(processed_images_dir)\n",
    "\n",
    "get_landmarks_dir(processed_images_dir,landmarks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \"./checkpoints/stylegan3-r-ffhq-1024_module.pkl\"... Done.\n",
      "()\n",
      "{'z_dim': 512, 'w_dim': 512, 'channel_base': 65536, 'channel_max': 1024, 'mapping_kwargs': {}, 'conv_kernel': 1, 'filter_size': 6, 'magnitude_ema_beta': 0.9988915792636801, 'output_scale': 0.25, 'c_dim': 0, 'img_resolution': 1024, 'img_channels': 3, 'use_radial_filters': True}\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n"
     ]
    }
   ],
   "source": [
    "# running draggan\n",
    "\n",
    "# visualizer_auto.py\n",
    "# - cuda device setting\n",
    "# - pretrained_stylegan_weights path -> gloabl_state()\n",
    "\n",
    "# viz/renderer.py\n",
    "# - landmark points resolution error fixes\n",
    "\n",
    "# visualizer_experiment.ipynb\n",
    "import os\n",
    "DragGAN_dir = '/home/bean/DragVideo/DragGAN'\n",
    "os.chdir('/home/bean/DragVideo/DragGAN/') # draggan folder absolute\n",
    "\n",
    "from auto_drag import do_drag\n",
    "from auto_drag import modify_landmarks\n",
    "\n",
    "def get_arguments(name,\n",
    "                  N_STEPS=100,\n",
    "                  CHECKPOINT_PATH=None): # exp_name\n",
    "                  #basedir+exp_name\n",
    "    landmarks_path = f\"/home/bean/DragVideo/Data_store/data/PTI_results/landmarks/{name}.pkl\"\n",
    "    return {\n",
    "        'w_load_path':f\"/home/bean/DragVideo/Data_store/data/PTI_results/embeddings/barcelona/PTI/{name}/0.pt\",\n",
    "        'stylegan2_wieghts_path' : CHECKPOINT_PATH,\n",
    "\n",
    "        'points' : modify_landmarks(landmarks_path),\n",
    "        'N_STEPS': N_STEPS,\n",
    "        'save_path':f\"/home/bean/DragVideo/Data_store/data/PTI_results/after_drag/{name}.png\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pkl = \"/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints/stylegan2_JFYUNISDNAIO.pkl\"\n",
    "old_pkl = \"/home/bean/DragVideo/DragGAN/PTI_results/checkpoints/stylegan3_IZRDVTQHLVHZ.pkl\"\n",
    "# import pickle \n",
    "with open(new_pkl,'rb') as f:\n",
    "    new = pickle.load(f)\n",
    "with open(old_pkl,'rb') as f:\n",
    "    old = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg2_old_e = '/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints/stylegan2_QOVKAIMEFUBS.pkl'\n",
    "sg2_new_e = '/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints/stylegan2_WNTSKGDZTGZX.pkl'\n",
    "\n",
    "sg3_finetuned = '/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints/stylegan3_HCQDSNSTNNJM.pkl'\n",
    "sg3_org_256 = '/home/bean/DragVideo/DragGAN/checkpoints/stylegan3-r-ffhqu-256x256.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['G', 'G_ema', 'D', 'training_set_kwargs', 'augment_pipe']),\n",
       " dict_keys(['G', 'G_ema', 'D', 'training_set_kwargs', 'augment_pipe']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.keys(), old.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['G_ema'],old['G_ema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intiating global state....\n",
      "calling init_images......\n",
      "Loading \"/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints/stylegan3_TQVCVHVTJBUI.pkl\"... Done.\n",
      "()\n",
      "{'z_dim': 512, 'w_dim': 512, 'channel_base': 65536, 'channel_max': 1024, 'mapping_kwargs': {}, 'conv_kernel': 1, 'filter_size': 6, 'magnitude_ema_beta': 0.9988915792636801, 'output_scale': 0.25, 'c_dim': 0, 'img_resolution': 1024, 'img_channels': 3, 'use_radial_filters': True}\n",
      "Running with:\n",
      "    Source: [array([301, 428]), array([304, 498]), array([311, 568]), array([319, 640]), array([340, 705]), array([373, 768]), array([407, 831]), array([451, 886]), array([510, 902]), array([572, 889]), array([622, 839]), array([663, 782]), array([702, 720]), array([727, 654]), array([738, 582]), array([746, 507]), array([751, 433]), array([331, 372]), array([347, 332]), array([385, 309]), array([429, 306]), array([471, 320]), array([547, 314]), array([592, 299]), array([638, 305]), array([677, 331]), array([698, 372]), array([507, 393]), array([506, 450]), array([504, 505]), array([502, 562]), array([456, 589]), array([479, 601]), array([506, 614]), array([534, 600]), array([560, 587]), array([382, 417]), array([404, 406]), array([431, 405]), array([454, 413]), array([430, 417]), array([405, 419]), array([571, 411]), array([595, 402]), array([620, 403]), array([640, 413]), array([619, 416]), array([595, 414]), array([424, 683]), array([448, 668]), array([480, 664]), array([509, 671]), array([538, 662]), array([574, 667]), array([606, 681]), array([579, 739]), array([543, 764]), array([511, 768]), array([479, 765]), array([445, 741]), array([436, 685]), array([480, 682]), array([509, 685]), array([538, 681]), array([594, 685]), array([542, 731]), array([510, 737]), array([480, 731])]\n",
      "    Target: [array([251, 428]), array([254, 498]), array([261, 568]), array([269, 640]), array([290, 705]), array([323, 768]), array([407, 781]), array([451, 836]), array([510, 852]), array([572, 839]), array([622, 789]), array([713, 782]), array([752, 720]), array([777, 654]), array([788, 582]), array([796, 507]), array([801, 433]), array([331, 372]), array([347, 332]), array([385, 309]), array([429, 306]), array([471, 320]), array([547, 314]), array([592, 299]), array([638, 305]), array([677, 331]), array([698, 372]), array([507, 393]), array([506, 450]), array([504, 505]), array([502, 562]), array([506, 614]), array([506, 614]), array([506, 614]), array([506, 614]), array([506, 614]), array([382, 417]), array([404, 406]), array([431, 405]), array([454, 413]), array([430, 417]), array([405, 419]), array([571, 411]), array([595, 402]), array([620, 403]), array([640, 413]), array([619, 416]), array([595, 414]), array([424, 683]), array([448, 668]), array([480, 664]), array([509, 671]), array([538, 662]), array([574, 667]), array([606, 681]), array([579, 739]), array([543, 764]), array([511, 768]), array([479, 765]), array([445, 741]), array([436, 685]), array([480, 682]), array([509, 685]), array([538, 681]), array([594, 685]), array([542, 731]), array([510, 737]), array([480, 731])]\n",
      "p_to_opt: [[428, 301], [498, 304], [568, 311], [640, 319], [705, 340], [768, 373], [831, 407], [886, 451], [902, 510], [889, 572], [839, 622], [782, 663], [720, 702], [654, 727], [582, 738], [507, 746], [433, 751], [372, 331], [332, 347], [309, 385], [306, 429], [320, 471], [314, 547], [299, 592], [305, 638], [331, 677], [372, 698], [393, 507], [450, 506], [505, 504], [562, 502], [589, 456], [601, 479], [614, 506], [600, 534], [587, 560], [417, 382], [406, 404], [405, 431], [413, 454], [417, 430], [419, 405], [411, 571], [402, 595], [403, 620], [413, 640], [416, 619], [414, 595], [683, 424], [668, 448], [664, 480], [671, 509], [662, 538], [667, 574], [681, 606], [739, 579], [764, 543], [768, 511], [765, 479], [741, 445], [685, 436], [682, 480], [685, 509], [681, 538], [685, 594], [731, 542], [737, 510], [731, 480]]\n",
      "t_to_opt: [[428, 251], [498, 254], [568, 261], [640, 269], [705, 290], [768, 323], [781, 407], [836, 451], [852, 510], [839, 572], [789, 622], [782, 713], [720, 752], [654, 777], [582, 788], [507, 796], [433, 801], [372, 331], [332, 347], [309, 385], [306, 429], [320, 471], [314, 547], [299, 592], [305, 638], [331, 677], [372, 698], [393, 507], [450, 506], [505, 504], [562, 502], [614, 506], [614, 506], [614, 506], [614, 506], [614, 506], [417, 382], [406, 404], [405, 431], [413, 454], [417, 430], [419, 405], [411, 571], [402, 595], [403, 620], [413, 640], [416, 619], [414, 595], [683, 424], [668, 448], [664, 480], [671, 509], [662, 538], [667, 574], [681, 606], [739, 579], [764, 543], [768, 511], [765, 479], [741, 445], [685, 436], [682, 480], [685, 509], [681, 538], [685, 594], [731, 542], [737, 510], [731, 480]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bean/.conda/envs/stylegan3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 10.76 GiB total capacity; 6.15 GiB already allocated; 2.37 GiB free; 6.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m names:\n\u001b[1;32m     17\u001b[0m     args \u001b[39m=\u001b[39m get_arguments(name,CHECKPOINT_PATH \u001b[39m=\u001b[39mTUNED_STYLEGAN3_WEIGHTS_PATH,N_STEPS\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     do_drag(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/auto_drag.py:16\u001b[0m, in \u001b[0;36mdo_drag\u001b[0;34m(w_load_path, stylegan2_wieghts_path, points, N_STEPS, save_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# from visualizer_auto import DragVideo\u001b[39;00m\n\u001b[1;32m     13\u001b[0m drag_video \u001b[39m=\u001b[39m DragVideo(w_load\u001b[39m=\u001b[39mw_load,\n\u001b[1;32m     14\u001b[0m                     stylegan2_wieghts_path\u001b[39m=\u001b[39mstylegan2_wieghts_path)\n\u001b[0;32m---> 16\u001b[0m feat \u001b[39m=\u001b[39m drag_video\u001b[39m.\u001b[39;49mrun(N_STEPS\u001b[39m=\u001b[39;49mN_STEPS,points\u001b[39m=\u001b[39;49mpoints)\n\u001b[1;32m     17\u001b[0m image \u001b[39m=\u001b[39m drag_video\u001b[39m.\u001b[39mglobal_state[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mimage_raw\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m save_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/visualizer_auto.py:483\u001b[0m, in \u001b[0;36mDragVideo.run\u001b[0;34m(self, N_STEPS, points)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m,N_STEPS \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,points \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_feature_map \u001b[39m=\u001b[39m  on_click_start(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_state,\n\u001b[1;32m    484\u001b[0m             N_STEPS \u001b[39m=\u001b[39;49m N_STEPS,\n\u001b[1;32m    485\u001b[0m             points \u001b[39m=\u001b[39;49m points,\n\u001b[1;32m    486\u001b[0m              )\n\u001b[1;32m    487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_feature_map,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_state[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mimage_show\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/visualizer_auto.py:307\u001b[0m, in \u001b[0;36mon_click_start\u001b[0;34m(global_state, N_STEPS, points)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39m# do drage here!\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m feature_map \u001b[39m=\u001b[39m renderer\u001b[39m.\u001b[39;49m_render_drag_impl(\n\u001b[1;32m    308\u001b[0m     global_state[\u001b[39m'\u001b[39;49m\u001b[39mgenerator_params\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    309\u001b[0m     p_to_opt,  \u001b[39m# point\u001b[39;49;00m\n\u001b[1;32m    310\u001b[0m     t_to_opt,  \u001b[39m# target\u001b[39;49;00m\n\u001b[1;32m    311\u001b[0m     drag_mask,  \u001b[39m# mask,\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m     global_state[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mmotion_lambda\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m# lambda_mask\u001b[39;49;00m\n\u001b[1;32m    313\u001b[0m     reg\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    314\u001b[0m     feature_idx\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,  \u001b[39m# NOTE: do not support change for now\u001b[39;49;00m\n\u001b[1;32m    315\u001b[0m     r1\u001b[39m=\u001b[39;49mglobal_state[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mr1_in_pixels\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m# r1\u001b[39;49;00m\n\u001b[1;32m    316\u001b[0m     r2\u001b[39m=\u001b[39;49mglobal_state[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mr2_in_pixels\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m# r2\u001b[39;49;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# random_seed     = 0,\u001b[39;49;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39m# noise_mode      = 'const',\u001b[39;49;00m\n\u001b[1;32m    319\u001b[0m     trunc_psi\u001b[39m=\u001b[39;49mglobal_state[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtrunc_psi\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    320\u001b[0m     \u001b[39m# force_fp32      = False,\u001b[39;49;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m# layer_name      = None,\u001b[39;49;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39m# sel_channels    = 3,\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m     \u001b[39m# base_channel    = 0,\u001b[39;49;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39m# img_scale_db    = 0,\u001b[39;49;00m\n\u001b[1;32m    325\u001b[0m     \u001b[39m# img_normalize   = False,\u001b[39;49;00m\n\u001b[1;32m    326\u001b[0m     \u001b[39m# untransform     = False,\u001b[39;49;00m\n\u001b[1;32m    327\u001b[0m     is_drag\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    328\u001b[0m     to_pil\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m step_idx \u001b[39m%\u001b[39m global_state[\u001b[39m'\u001b[39m\u001b[39mdraw_interval\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    331\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCurrent Source:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/viz/renderer.py:385\u001b[0m, in \u001b[0;36mRenderer._render_drag_impl\u001b[0;34m(self, res, points, targets, mask, lambda_mask, reg, feature_idx, r1, r2, random_seed, noise_mode, trunc_psi, force_fp32, layer_name, sel_channels, base_channel, img_scale_db, img_normalize, untransform, is_drag, reset, to_pil, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m         gridw \u001b[39m=\u001b[39m (reljs\u001b[39m-\u001b[39mdirection[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m (w\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    384\u001b[0m         grid \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([gridw,gridh], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 385\u001b[0m         target \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgrid_sample(feat_resize\u001b[39m.\u001b[39;49mfloat(), grid, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n\u001b[1;32m    386\u001b[0m         loss_motion \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m F\u001b[39m.\u001b[39ml1_loss(feat_resize[:,:,relis,reljs], target\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m    388\u001b[0m loss \u001b[39m=\u001b[39m loss_motion\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 10.76 GiB total capacity; 6.15 GiB already allocated; 2.37 GiB free; 6.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "latents_dir = \"/home/bean/DragVideo/Data_store/data/PTI_results/embeddings/barcelona/PTI\"\n",
    "\n",
    "#  run draggan on based on latents availability\n",
    "\n",
    "temp = os.listdir(latents_dir)\n",
    "temp.sort()\n",
    "names = [i.split('.')[0] for i in temp]\n",
    "# names\n",
    "\n",
    "# path to the desired stylegan finetuned weights from PTI\n",
    "TUNED_STYLEGAN3_WEIGHTS_PATH = \"/home/bean/DragVideo/Data_store/data/PTI_results/checkpoints/stylegan3_TQVCVHVTJBUI.pkl\"\n",
    "\n",
    "for name in names:\n",
    "    args = get_arguments(name,CHECKPOINT_PATH =TUNED_STYLEGAN3_WEIGHTS_PATH,N_STEPS=1)\n",
    "    do_drag(**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
