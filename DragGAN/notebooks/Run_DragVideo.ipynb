{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. paths can be set from here.\n",
    "2. all the paths are inside the dragvideo container\n",
    "3. set kernel : Dragvideo env "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#            UTILITY FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "import os \n",
    "\n",
    "def add_dummy_config(filename,text,ROOT_PATH = '.',mode = \"w\"):\n",
    "    file_path = os.path.join(ROOT_PATH, filename)\n",
    "    with open(file_path, mode) as f:\n",
    "        f.write(text)\n",
    "        \n",
    "\n",
    "def generate_text(**kwargs):\n",
    "    text = \"\" \n",
    "    for key, value in kwargs.items():\n",
    "        if type(value) == str:\n",
    "            value = f\"'{value}'\"\n",
    "        text += f\"{key}= {value}\\n\"\n",
    "    return text\n",
    "\n",
    "def add_dummy_config_from_dict(filename,dict_,ROOT_PATH = '.',mode = \"w\"):\n",
    "    text = generate_text(**dict_)\n",
    "    add_dummy_config(filename,text,ROOT_PATH = ROOT_PATH,mode = mode)\n",
    "    \n",
    "    \n",
    "# ========================================\n",
    "def import_module_from_path( path,name='_module'):\n",
    "    # ex: path = \"path/to/module.py\"\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(name, path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set paths here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> NOTE:</b>  \n",
    "To update the configs. this file changes must happen before import.\n",
    "restart <i>to not import </i> from cached files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "video_utils = import_module_from_path( \"/home/bean/DragVideo/DragGAN/utils/video_utils.py\")\n",
    "\n",
    "# set path to PTI folder\n",
    "PTI_PATH = '/home/bean/DragVideo/DragGAN/_PTI'\n",
    "dummy_config_path = os.path.join(PTI_PATH, \"configs\",\"dummy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input experiment path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create experiment data folder structure\n",
    "Experiment_name = 'exp_01_man'\n",
    "Experiment_base_path = '/home/bean/DragVideo/Data_store/experiments/' \n",
    "Experiment_path = os.path.join(Experiment_base_path, Experiment_name)\n",
    "\n",
    "\n",
    "import subprocess\n",
    "os.chdir(PTI_PATH)   # .sh file is in PTI_PATH\n",
    "subprocess.call(['sh', 'init_datadirs.sh', Experiment_name, Experiment_base_path]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path configs , hyperparameters \n",
    "paths_config_dict = {\n",
    "    #pretrained models\n",
    "    \"e4e\": \"/home/bean/DragVideo/DragGAN/_PTI/pretrained_models/e4e_ffhq_encode.pt\",\n",
    "    \"stylegan2_ada_ffhq\": \"/home/bean/DragVideo/DragGAN/_PTI/pretrained_models/ffhq.pkl\",\n",
    "    \n",
    "    # to store tuned stylegan weights\n",
    "    \"checkpoints_dir\": os.path.join(Experiment_path,'tuned_SG'),\n",
    "    # to store latents\n",
    "    \"embedding_base_dir\": os.path.join(Experiment_path,'latents'),\n",
    "    # aligned / processed images\n",
    "    \"input_data_path\": os.path.join(Experiment_path,'aligned'),\n",
    "}\n",
    "\n",
    "add_dummy_config_from_dict(\"paths_config.py\", paths_config_dict,ROOT_PATH=dummy_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:11<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# get frames \n",
    "video_path = \"/home/bean/DragVideo/Data_store/OLD/original_videos/person_speaking_original.mp4\"\n",
    "raw_path = os.path.join(Experiment_path, \"raw\")\n",
    "\n",
    "if False:\n",
    "        video_utils.extract_frames(video_path, raw_path)\n",
    "\n",
    "# align images\n",
    "if False:\n",
    "    os.chdir(PTI_PATH)\n",
    "    from utils.align_data import pre_process_images\n",
    "    pre_process_images(raw_path, IMAGE_SIZE=1024) # o/p: config.input_data_path\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running PTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bean/DragVideo/env/Dragvideo/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bean/DragVideo/env/Dragvideo/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/bean/DragVideo/env/Dragvideo/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "current_directory:  /home/bean/DragVideo/DragGAN/_PTI\n",
      "current_directory:  /home/bean/DragVideo/DragGAN/_PTI\n",
      "torch.Size([3, 1024, 1024]) 3 1024 1024\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 107, in forward\n      _48 = torch.__not__(torch.__contains__(_47, \"conv1\"))\n      if _48:\n        x5 = (_22).forward(x4, )\n              ~~~~~~~~~~~~ <--- HERE\n      else:\n        x5 = x4\n  File \"code/__torch__.py\", line 447, in forward\n    _192 = torch.to(self.bias, ops.prim.dtype(x), False, False, None)\n    _193 = self.padding\n    x49 = torch.conv2d(x, _191, _192, [1, 1], [_193, _193], [1, 1], 1)\n          ~~~~~~~~~~~~ <--- HERE\n    x50 = __torch__.torch.nn.functional.relu(x49, False, )\n    return x50\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 103, in forward\n                    ys.append(x.to(torch.float32))\n                if name not in ['pool5', 'fc1', 'fc2', 'fc3', 'softmax']:\n                    x = layer(x)\n                        ~~~~~ <--- HERE\n            for idx, (y, w) in enumerate(zip(ys, [self.lpips0, self.lpips1, self.lpips2, self.lpips3, self.lpips4])):\n                y = y / (torch.norm(y, dim=1, keepdim=True) + 1e-10)\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 18, in forward\n    def forward(self, x):\n        x = torch.nn.functional.conv2d(x, self.weight.to(x.dtype), self.bias.to(x.dtype), padding=self.padding)\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        x = torch.nn.functional.relu(x)\n        return x\nRuntimeError: GET was unable to find an engine to execute this computation\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m use_multi_id_training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun_pti\u001b[39;00m \u001b[39mimport\u001b[39;00m run_PTI\n\u001b[0;32m----> 6\u001b[0m model_id \u001b[39m=\u001b[39m run_PTI(use_wandb\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, use_multi_id_training\u001b[39m=\u001b[39;49muse_multi_id_training)\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/_PTI/scripts/run_pti.py:42\u001b[0m, in \u001b[0;36mrun_PTI\u001b[0;34m(run_name, use_wandb, use_multi_id_training)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     coach \u001b[39m=\u001b[39m SingleIDCoach(dataloader, use_wandb)\n\u001b[0;32m---> 42\u001b[0m coach\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m global_config\u001b[39m.\u001b[39mrun_name\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/_PTI/training/coaches/multi_id_coach.py:39\u001b[0m, in \u001b[0;36mMultiIDCoach.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m     embedding_dir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mw_path_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mpaths_config\u001b[39m.\u001b[39mpti_results_keyword\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mimage_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     37\u001b[0m os\u001b[39m.\u001b[39mmakedirs(embedding_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 39\u001b[0m w_pivot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_inversion(w_path_dir, image_name, image)\n\u001b[1;32m     40\u001b[0m w_pivots\u001b[39m.\u001b[39mappend(w_pivot)\n\u001b[1;32m     41\u001b[0m images\u001b[39m.\u001b[39mappend((image_name, image))\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/_PTI/training/coaches/base_coach.py:66\u001b[0m, in \u001b[0;36mBaseCoach.get_inversion\u001b[0;34m(self, w_path_dir, image_name, image)\u001b[0m\n\u001b[1;32m     63\u001b[0m     w_pivot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_inversions(w_path_dir, image_name)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m hyperparameters\u001b[39m.\u001b[39muse_last_w_pivots \u001b[39mor\u001b[39;00m w_pivot \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     w_pivot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalc_inversions(image, image_name)\n\u001b[1;32m     67\u001b[0m     torch\u001b[39m.\u001b[39msave(w_pivot, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00membedding_dir\u001b[39m}\u001b[39;00m\u001b[39m/0.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m w_pivot \u001b[39m=\u001b[39m w_pivot\u001b[39m.\u001b[39mto(global_config\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/_PTI/training/coaches/base_coach.py:93\u001b[0m, in \u001b[0;36mBaseCoach.calc_inversions\u001b[0;34m(self, image, image_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     id_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze((image\u001b[39m.\u001b[39mto(global_config\u001b[39m.\u001b[39mdevice) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m---> 93\u001b[0m     w \u001b[39m=\u001b[39m w_projector\u001b[39m.\u001b[39;49mproject(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mG, id_image, device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(global_config\u001b[39m.\u001b[39;49mdevice), w_avg_samples\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m,\n\u001b[1;32m     94\u001b[0m                             num_steps\u001b[39m=\u001b[39;49mhyperparameters\u001b[39m.\u001b[39;49mfirst_inv_steps, w_name\u001b[39m=\u001b[39;49mimage_name,\n\u001b[1;32m     95\u001b[0m                             use_wandb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_wandb)\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m w\n",
      "File \u001b[0;32m~/DragVideo/DragGAN/_PTI/training/projectors/w_projector.py:73\u001b[0m, in \u001b[0;36mproject\u001b[0;34m(G, target, num_steps, w_avg_samples, initial_learning_rate, initial_noise_factor, lr_rampdown_length, lr_rampup_length, noise_ramp_length, regularize_noise_weight, verbose, device, use_wandb, initial_w, image_log_step, w_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m target_images\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m256\u001b[39m:\n\u001b[1;32m     72\u001b[0m     target_images \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(target_images, size\u001b[39m=\u001b[39m(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marea\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m target_features \u001b[39m=\u001b[39m vgg16(target_images, resize_images\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_lpips\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     75\u001b[0m w_opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(start_w, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m     76\u001b[0m                      requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m     77\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([w_opt] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(noise_bufs\u001b[39m.\u001b[39mvalues()), betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m),\n\u001b[1;32m     78\u001b[0m                              lr\u001b[39m=\u001b[39mhyperparameters\u001b[39m.\u001b[39mfirst_inv_lr)\n",
      "File \u001b[0;32m~/DragVideo/env/Dragvideo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 107, in forward\n      _48 = torch.__not__(torch.__contains__(_47, \"conv1\"))\n      if _48:\n        x5 = (_22).forward(x4, )\n              ~~~~~~~~~~~~ <--- HERE\n      else:\n        x5 = x4\n  File \"code/__torch__.py\", line 447, in forward\n    _192 = torch.to(self.bias, ops.prim.dtype(x), False, False, None)\n    _193 = self.padding\n    x49 = torch.conv2d(x, _191, _192, [1, 1], [_193, _193], [1, 1], 1)\n          ~~~~~~~~~~~~ <--- HERE\n    x50 = __torch__.torch.nn.functional.relu(x49, False, )\n    return x50\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 103, in forward\n                    ys.append(x.to(torch.float32))\n                if name not in ['pool5', 'fc1', 'fc2', 'fc3', 'softmax']:\n                    x = layer(x)\n                        ~~~~~ <--- HERE\n            for idx, (y, w) in enumerate(zip(ys, [self.lpips0, self.lpips1, self.lpips2, self.lpips3, self.lpips4])):\n                y = y / (torch.norm(y, dim=1, keepdim=True) + 1e-10)\n  File \"c:\\p4research\\research\\tkarras\\dnn\\gan3support\\feature_detectors\\vgg.py\", line 18, in forward\n    def forward(self, x):\n        x = torch.nn.functional.conv2d(x, self.weight.to(x.dtype), self.bias.to(x.dtype), padding=self.padding)\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        x = torch.nn.functional.relu(x)\n        return x\nRuntimeError: GET was unable to find an engine to execute this computation\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PTI_PATH)\n",
    "\n",
    "use_multi_id_training = True\n",
    "\n",
    "from scripts.run_pti import run_PTI\n",
    "model_id = run_PTI(use_wandb=False, use_multi_id_training=use_multi_id_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
